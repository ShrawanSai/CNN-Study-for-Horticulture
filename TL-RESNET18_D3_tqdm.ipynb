{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "import cv2\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 16\n",
      "Classes: ['calendula', 'bellflower', 'iris', 'astilbe', 'common_daisy', 'california_poppy', 'carnation', 'rose', 'tulip', 'black_eyed_susan', 'coreopsis', 'dandelion', 'water_lily', 'magnolia', 'sunflower', 'daffodil']\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/Users/sharanyu/Desktop/AIproject/flowers\"  # Change this to the path of the dataset on your system\n",
    "classes = os.listdir(data_dir)\n",
    "num_classes = len(classes)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Classes: {classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/Users/sharanyu/Desktop/AIproject/flowers\"\n",
    "class_names = os.listdir(data_dir)\n",
    "num_class = len(class_names)\n",
    "image_files = [[os.path.join(data_dir, class_name, x) \n",
    "               for x in os.listdir(os.path.join(data_dir, class_name))] \n",
    "               for class_name in class_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Class  NumFiles  AvgWidth  AvgHeight\n",
      "0          calendula       978       256        256\n",
      "1         bellflower       873       256        256\n",
      "2               iris      1054       256        256\n",
      "3            astilbe       737       256        256\n",
      "4       common_daisy       980       256        256\n",
      "5   california_poppy      1022       256        256\n",
      "6          carnation       923       256        256\n",
      "7               rose       999       256        256\n",
      "8              tulip      1048       256        256\n",
      "9   black_eyed_susan      1000       256        256\n",
      "10         coreopsis      1047       256        256\n",
      "11         dandelion      1052       256        256\n",
      "12        water_lily       982       781        586\n",
      "13          magnolia      1048       256        256\n",
      "14         sunflower      1027       256        256\n",
      "15          daffodil       970       256        256\n",
      "Total files:  15740\n"
     ]
    }
   ],
   "source": [
    "# analyzing the dataset, determining the no. of classes, dimesnions and files/class.\n",
    "rows = []\n",
    "file_count = 0\n",
    "for class_name in classes:\n",
    "    class_dir = os.path.join(data_dir, class_name)\n",
    "    num_files = len(os.listdir(class_dir))\n",
    "    total_width = 0\n",
    "    total_height = 0\n",
    "    \n",
    "    for img_name in os.listdir(class_dir):\n",
    "        try:\n",
    "          img_path = os.path.join(class_dir, img_name)\n",
    "          img = cv2.imread(img_path)\n",
    "          height, width, channels = img.shape\n",
    "          total_width += width\n",
    "          total_height += height\n",
    "          file_count += 1\n",
    "        except Exception as e:\n",
    "          print(img_path)\n",
    "          print(str(e))\n",
    "    avg_width = total_width // num_files\n",
    "    avg_height = total_height // num_files\n",
    "    rows.append({\"Class\": class_name, \"NumFiles\": num_files, \"AvgWidth\": avg_width, \"AvgHeight\": avg_height})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(df)\n",
    "print(\"Total files: \",file_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 25\n",
    "batch_size = 128\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation for the train data.\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),    \n",
    "    transforms.RandomRotation(10),        \n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), \n",
    "    transforms.RandomHorizontalFlip(),  \n",
    "    transforms.RandomVerticalFlip(),     \n",
    "    transforms.RandomPerspective(distortion_scale=0.2, p=0.2),  \n",
    "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                          [0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFolder(root='flowers', transform=train_transform)\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# creating dataloaders with bathc size of 128\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.resnet18(pretrained = True) # resnet transfer learning for dataset 3\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the last layer of the fully connected part of the network\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/25: 100%|██████████| 87/87 [01:07<00:00,  1.29it/s]\n",
      "Validation Epoch 1/25: 100%|██████████| 13/13 [00:10<00:00,  1.29it/s]\n",
      "Test Epoch 1/25: 100%|██████████| 25/25 [00:19<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Train Loss: 2.7542, Train Accuracy: 10.83%, Val Accuracy: 17.15%, Test Accuracy: 16.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/25: 100%|██████████| 87/87 [01:07<00:00,  1.29it/s]\n",
      "Validation Epoch 2/25: 100%|██████████| 13/13 [00:10<00:00,  1.28it/s]\n",
      "Test Epoch 2/25: 100%|██████████| 25/25 [00:20<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25, Train Loss: 2.4649, Train Accuracy: 25.51%, Val Accuracy: 33.48%, Test Accuracy: 32.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/25: 100%|██████████| 87/87 [01:07<00:00,  1.30it/s]\n",
      "Validation Epoch 3/25: 100%|██████████| 13/13 [00:09<00:00,  1.41it/s]\n",
      "Test Epoch 3/25: 100%|██████████| 25/25 [00:18<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25, Train Loss: 2.2254, Train Accuracy: 38.89%, Val Accuracy: 43.33%, Test Accuracy: 45.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/25: 100%|██████████| 87/87 [01:07<00:00,  1.30it/s]\n",
      "Validation Epoch 4/25: 100%|██████████| 13/13 [00:09<00:00,  1.32it/s]\n",
      "Test Epoch 4/25: 100%|██████████| 25/25 [00:19<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25, Train Loss: 2.0350, Train Accuracy: 47.90%, Val Accuracy: 50.89%, Test Accuracy: 50.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/25: 100%|██████████| 87/87 [01:06<00:00,  1.31it/s]\n",
      "Validation Epoch 5/25: 100%|██████████| 13/13 [00:09<00:00,  1.39it/s]\n",
      "Test Epoch 5/25: 100%|██████████| 25/25 [00:18<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25, Train Loss: 1.8845, Train Accuracy: 53.45%, Val Accuracy: 56.80%, Test Accuracy: 54.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/25: 100%|██████████| 87/87 [01:06<00:00,  1.30it/s]\n",
      "Validation Epoch 6/25: 100%|██████████| 13/13 [00:09<00:00,  1.40it/s]\n",
      "Test Epoch 6/25: 100%|██████████| 25/25 [00:18<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25, Train Loss: 1.7563, Train Accuracy: 57.15%, Val Accuracy: 61.82%, Test Accuracy: 60.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/25: 100%|██████████| 87/87 [01:05<00:00,  1.33it/s]\n",
      "Validation Epoch 7/25: 100%|██████████| 13/13 [00:09<00:00,  1.41it/s]\n",
      "Test Epoch 7/25: 100%|██████████| 25/25 [00:18<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25, Train Loss: 1.6489, Train Accuracy: 59.15%, Val Accuracy: 60.42%, Test Accuracy: 59.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/25: 100%|██████████| 87/87 [01:05<00:00,  1.33it/s]\n",
      "Validation Epoch 8/25: 100%|██████████| 13/13 [00:09<00:00,  1.41it/s]\n",
      "Test Epoch 8/25: 100%|██████████| 25/25 [00:18<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/25, Train Loss: 1.5702, Train Accuracy: 61.67%, Val Accuracy: 62.39%, Test Accuracy: 62.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/25: 100%|██████████| 87/87 [01:04<00:00,  1.34it/s]\n",
      "Validation Epoch 9/25: 100%|██████████| 13/13 [00:09<00:00,  1.41it/s]\n",
      "Test Epoch 9/25: 100%|██████████| 25/25 [00:18<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25, Train Loss: 1.4865, Train Accuracy: 63.29%, Val Accuracy: 63.85%, Test Accuracy: 63.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/25: 100%|██████████| 87/87 [01:05<00:00,  1.33it/s]\n",
      "Validation Epoch 10/25: 100%|██████████| 13/13 [00:09<00:00,  1.41it/s]\n",
      "Test Epoch 10/25: 100%|██████████| 25/25 [00:18<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25, Train Loss: 1.4205, Train Accuracy: 64.37%, Val Accuracy: 65.06%, Test Accuracy: 64.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11/25: 100%|██████████| 87/87 [01:04<00:00,  1.34it/s]\n",
      "Validation Epoch 11/25: 100%|██████████| 13/13 [00:09<00:00,  1.40it/s]\n",
      "Test Epoch 11/25: 100%|██████████| 25/25 [00:18<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25, Train Loss: 1.3845, Train Accuracy: 65.00%, Val Accuracy: 65.37%, Test Accuracy: 66.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12/25: 100%|██████████| 87/87 [01:05<00:00,  1.33it/s]\n",
      "Validation Epoch 12/25: 100%|██████████| 13/13 [00:09<00:00,  1.40it/s]\n",
      "Test Epoch 12/25: 100%|██████████| 25/25 [00:18<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/25, Train Loss: 1.3388, Train Accuracy: 65.92%, Val Accuracy: 65.06%, Test Accuracy: 66.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13/25: 100%|██████████| 87/87 [01:04<00:00,  1.34it/s]\n",
      "Validation Epoch 13/25: 100%|██████████| 13/13 [00:09<00:00,  1.39it/s]\n",
      "Test Epoch 13/25: 100%|██████████| 25/25 [00:18<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25, Train Loss: 1.2976, Train Accuracy: 66.48%, Val Accuracy: 65.37%, Test Accuracy: 67.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14/25: 100%|██████████| 87/87 [01:06<00:00,  1.32it/s]\n",
      "Validation Epoch 14/25: 100%|██████████| 13/13 [00:09<00:00,  1.41it/s]\n",
      "Test Epoch 14/25: 100%|██████████| 25/25 [00:18<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25, Train Loss: 1.2558, Train Accuracy: 66.26%, Val Accuracy: 66.65%, Test Accuracy: 66.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15/25: 100%|██████████| 87/87 [01:05<00:00,  1.33it/s]\n",
      "Validation Epoch 15/25: 100%|██████████| 13/13 [00:09<00:00,  1.39it/s]\n",
      "Test Epoch 15/25: 100%|██████████| 25/25 [00:18<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25, Train Loss: 1.2237, Train Accuracy: 67.05%, Val Accuracy: 66.20%, Test Accuracy: 67.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16/25: 100%|██████████| 87/87 [01:06<00:00,  1.31it/s]\n",
      "Validation Epoch 16/25: 100%|██████████| 13/13 [00:09<00:00,  1.38it/s]\n",
      "Test Epoch 16/25: 100%|██████████| 25/25 [00:18<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25, Train Loss: 1.2021, Train Accuracy: 67.28%, Val Accuracy: 66.14%, Test Accuracy: 66.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17/25: 100%|██████████| 87/87 [01:06<00:00,  1.31it/s]\n",
      "Validation Epoch 17/25: 100%|██████████| 13/13 [00:09<00:00,  1.38it/s]\n",
      "Test Epoch 17/25: 100%|██████████| 25/25 [00:19<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/25, Train Loss: 1.1819, Train Accuracy: 68.48%, Val Accuracy: 67.73%, Test Accuracy: 67.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18/25: 100%|██████████| 87/87 [01:06<00:00,  1.32it/s]\n",
      "Validation Epoch 18/25: 100%|██████████| 13/13 [00:09<00:00,  1.37it/s]\n",
      "Test Epoch 18/25: 100%|██████████| 25/25 [00:18<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/25, Train Loss: 1.1479, Train Accuracy: 68.52%, Val Accuracy: 68.23%, Test Accuracy: 68.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19/25: 100%|██████████| 87/87 [01:06<00:00,  1.31it/s]\n",
      "Validation Epoch 19/25: 100%|██████████| 13/13 [00:09<00:00,  1.33it/s]\n",
      "Test Epoch 19/25: 100%|██████████| 25/25 [00:19<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/25, Train Loss: 1.1325, Train Accuracy: 68.73%, Val Accuracy: 68.23%, Test Accuracy: 69.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20/25: 100%|██████████| 87/87 [01:11<00:00,  1.21it/s]\n",
      "Validation Epoch 20/25: 100%|██████████| 13/13 [00:10<00:00,  1.18it/s]\n",
      "Test Epoch 20/25: 100%|██████████| 25/25 [00:20<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/25, Train Loss: 1.1205, Train Accuracy: 69.12%, Val Accuracy: 67.98%, Test Accuracy: 68.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 21/25: 100%|██████████| 87/87 [01:07<00:00,  1.30it/s]\n",
      "Validation Epoch 21/25: 100%|██████████| 13/13 [00:09<00:00,  1.39it/s]\n",
      "Test Epoch 21/25: 100%|██████████| 25/25 [00:19<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/25, Train Loss: 1.0973, Train Accuracy: 69.39%, Val Accuracy: 68.42%, Test Accuracy: 70.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 22/25: 100%|██████████| 87/87 [01:09<00:00,  1.25it/s]\n",
      "Validation Epoch 22/25: 100%|██████████| 13/13 [00:09<00:00,  1.34it/s]\n",
      "Test Epoch 22/25: 100%|██████████| 25/25 [00:19<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/25, Train Loss: 1.0794, Train Accuracy: 69.74%, Val Accuracy: 70.14%, Test Accuracy: 70.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 23/25: 100%|██████████| 87/87 [01:13<00:00,  1.18it/s]\n",
      "Validation Epoch 23/25: 100%|██████████| 13/13 [00:11<00:00,  1.12it/s]\n",
      "Test Epoch 23/25: 100%|██████████| 25/25 [00:23<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/25, Train Loss: 1.0607, Train Accuracy: 69.99%, Val Accuracy: 69.25%, Test Accuracy: 69.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 24/25: 100%|██████████| 87/87 [01:08<00:00,  1.26it/s]\n",
      "Validation Epoch 24/25: 100%|██████████| 13/13 [00:09<00:00,  1.39it/s]\n",
      "Test Epoch 24/25: 100%|██████████| 25/25 [00:19<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/25, Train Loss: 1.0477, Train Accuracy: 70.25%, Val Accuracy: 69.06%, Test Accuracy: 69.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 25/25: 100%|██████████| 87/87 [01:09<00:00,  1.24it/s]\n",
      "Validation Epoch 25/25: 100%|██████████| 13/13 [00:09<00:00,  1.36it/s]\n",
      "Test Epoch 25/25: 100%|██████████| 25/25 [00:19<00:00,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25, Train Loss: 1.0401, Train Accuracy: 70.38%, Val Accuracy: 70.14%, Test Accuracy: 70.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "best_val_accuracy = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(tqdm(train_loader, desc=f'Training Epoch {epoch+1}/{num_epochs}')):\n",
    "        # Move data to device\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predictions = torch.max(scores.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predictions == targets).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_accuracy = 100 * correct / total\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in tqdm(val_loader, desc=f'Validation Epoch {epoch+1}/{num_epochs}'):\n",
    "\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            scores = model(data)\n",
    "\n",
    "            _, predictions = torch.max(scores.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predictions == targets).sum().item()\n",
    "\n",
    "        val_accuracy = 100 * correct / total\n",
    "\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in tqdm(test_loader, desc=f'Test Epoch {epoch+1}/{num_epochs}'):\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            scores = model(data)\n",
    "\n",
    "            _, predictions = torch.max(scores.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predictions == targets).sum().item()\n",
    "\n",
    "        test_accuracy = 100 * correct / total\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, '\n",
    "          f'Val Accuracy: {val_accuracy:.2f}%, '\n",
    "          f'Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    Epoch 1/25, Batch 0/79, Loss 2.9198\\n    Validation accuracy: 19.61%\\n    Test accuracy: 19.12%\\n    Epoch 2/25, Batch 0/79, Loss 2.5499\\n    Validation accuracy: 34.10%\\n    Test accuracy: 33.04%\\n    Epoch 3/25, Batch 0/79, Loss 2.3307\\n    Validation accuracy: 44.54%\\n    Test accuracy: 44.35%\\n    Epoch 4/25, Batch 0/79, Loss 2.1449\\n    Validation accuracy: 51.13%\\n    Test accuracy: 50.73%\\n    Epoch 5/25, Batch 0/79, Loss 2.0068\\n    Validation accuracy: 55.22%\\n    Test accuracy: 54.67%\\n    Epoch 6/25, Batch 0/79, Loss 1.8410\\n    Validation accuracy: 58.67%\\n    Test accuracy: 58.13%\\n    Epoch 7/25, Batch 0/79, Loss 1.6271\\n    Validation accuracy: 60.06%\\n    Test accuracy: 59.91%\\n    Epoch 8/25, Batch 0/79, Loss 1.5167\\n    Validation accuracy: 61.93%\\n    Test accuracy: 60.99%\\n    Epoch 9/25, Batch 0/79, Loss 1.5282\\n    Validation accuracy: 62.76%\\n    Test accuracy: 62.23%\\n    Epoch 10/25, Batch 0/79, Loss 1.3853\\n    Validation accuracy: 63.80%\\n    Test accuracy: 63.63%\\n    Epoch 11/25, Batch 0/79, Loss 1.4738\\n    Validation accuracy: 65.11%\\n    Test accuracy: 63.95%\\n    Epoch 12/25, Batch 0/79, Loss 1.3234\\n    Validation accuracy: 66.14%\\n    Test accuracy: 65.41%\\n    Epoch 13/25, Batch 0/79, Loss 1.3975\\n    Validation accuracy: 66.42%\\n    Test accuracy: 65.47%\\n    Epoch 14/25, Batch 0/79, Loss 1.3211\\n    Validation accuracy: 66.38%\\n    Test accuracy: 66.26%\\n    Epoch 15/25, Batch 0/79, Loss 1.2833\\n    Validation accuracy: 66.61%\\n    Test accuracy: 66.26%\\n    Epoch 16/25, Batch 0/79, Loss 1.2273\\n    Validation accuracy: 67.29%\\n    Test accuracy: 67.09%\\n    Epoch 17/25, Batch 0/79, Loss 1.1177\\n    Validation accuracy: 66.97%\\n    Test accuracy: 67.34%\\n    Epoch 18/25, Batch 0/79, Loss 1.0788\\n    Validation accuracy: 67.69%\\n    Test accuracy: 68.04%\\n    Epoch 19/25, Batch 0/79, Loss 1.2609\\n    Validation accuracy: 67.84%\\n    Test accuracy: 67.79%\\n    Epoch 20/25, Batch 0/79, Loss 1.0463\\n    Validation accuracy: 68.40%\\n    Test accuracy: 68.30%\\n    Epoch 21/25, Batch 0/79, Loss 1.0968\\n    Validation accuracy: 68.44%\\n    Test accuracy: 68.65%\\n    Epoch 22/25, Batch 0/79, Loss 1.0011\\n    Validation accuracy: 68.52%\\n    Test accuracy: 68.87%\\n    Epoch 23/25, Batch 0/79, Loss 1.3195\\n    Validation accuracy: 68.72%\\n    Test accuracy: 69.12%\\n    Epoch 24/25, Batch 0/79, Loss 0.9101\\n    Validation accuracy: 68.80%\\n    Test accuracy: 69.22%\\n    Epoch 25/25, Batch 0/79, Loss 0.9813\\n    Validation accuracy: 69.08%\\n    Test accuracy: 69.47%\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  initial run fullOutput:\n",
    "\n",
    "'''    Epoch 1/25, Batch 0/79, Loss 2.9198\n",
    "    Validation accuracy: 19.61%\n",
    "    Test accuracy: 19.12%\n",
    "    Epoch 2/25, Batch 0/79, Loss 2.5499\n",
    "    Validation accuracy: 34.10%\n",
    "    Test accuracy: 33.04%\n",
    "    Epoch 3/25, Batch 0/79, Loss 2.3307\n",
    "    Validation accuracy: 44.54%\n",
    "    Test accuracy: 44.35%\n",
    "    Epoch 4/25, Batch 0/79, Loss 2.1449\n",
    "    Validation accuracy: 51.13%\n",
    "    Test accuracy: 50.73%\n",
    "    Epoch 5/25, Batch 0/79, Loss 2.0068\n",
    "    Validation accuracy: 55.22%\n",
    "    Test accuracy: 54.67%\n",
    "    Epoch 6/25, Batch 0/79, Loss 1.8410\n",
    "    Validation accuracy: 58.67%\n",
    "    Test accuracy: 58.13%\n",
    "    Epoch 7/25, Batch 0/79, Loss 1.6271\n",
    "    Validation accuracy: 60.06%\n",
    "    Test accuracy: 59.91%\n",
    "    Epoch 8/25, Batch 0/79, Loss 1.5167\n",
    "    Validation accuracy: 61.93%\n",
    "    Test accuracy: 60.99%\n",
    "    Epoch 9/25, Batch 0/79, Loss 1.5282\n",
    "    Validation accuracy: 62.76%\n",
    "    Test accuracy: 62.23%\n",
    "    Epoch 10/25, Batch 0/79, Loss 1.3853\n",
    "    Validation accuracy: 63.80%\n",
    "    Test accuracy: 63.63%\n",
    "    Epoch 11/25, Batch 0/79, Loss 1.4738\n",
    "    Validation accuracy: 65.11%\n",
    "    Test accuracy: 63.95%\n",
    "    Epoch 12/25, Batch 0/79, Loss 1.3234\n",
    "    Validation accuracy: 66.14%\n",
    "    Test accuracy: 65.41%\n",
    "    Epoch 13/25, Batch 0/79, Loss 1.3975\n",
    "    Validation accuracy: 66.42%\n",
    "    Test accuracy: 65.47%\n",
    "    Epoch 14/25, Batch 0/79, Loss 1.3211\n",
    "    Validation accuracy: 66.38%\n",
    "    Test accuracy: 66.26%\n",
    "    Epoch 15/25, Batch 0/79, Loss 1.2833\n",
    "    Validation accuracy: 66.61%\n",
    "    Test accuracy: 66.26%\n",
    "    Epoch 16/25, Batch 0/79, Loss 1.2273\n",
    "    Validation accuracy: 67.29%\n",
    "    Test accuracy: 67.09%\n",
    "    Epoch 17/25, Batch 0/79, Loss 1.1177\n",
    "    Validation accuracy: 66.97%\n",
    "    Test accuracy: 67.34%\n",
    "    Epoch 18/25, Batch 0/79, Loss 1.0788\n",
    "    Validation accuracy: 67.69%\n",
    "    Test accuracy: 68.04%\n",
    "    Epoch 19/25, Batch 0/79, Loss 1.2609\n",
    "    Validation accuracy: 67.84%\n",
    "    Test accuracy: 67.79%\n",
    "    Epoch 20/25, Batch 0/79, Loss 1.0463\n",
    "    Validation accuracy: 68.40%\n",
    "    Test accuracy: 68.30%\n",
    "    Epoch 21/25, Batch 0/79, Loss 1.0968\n",
    "    Validation accuracy: 68.44%\n",
    "    Test accuracy: 68.65%\n",
    "    Epoch 22/25, Batch 0/79, Loss 1.0011\n",
    "    Validation accuracy: 68.52%\n",
    "    Test accuracy: 68.87%\n",
    "    Epoch 23/25, Batch 0/79, Loss 1.3195\n",
    "    Validation accuracy: 68.72%\n",
    "    Test accuracy: 69.12%\n",
    "    Epoch 24/25, Batch 0/79, Loss 0.9101\n",
    "    Validation accuracy: 68.80%\n",
    "    Test accuracy: 69.22%\n",
    "    Epoch 25/25, Batch 0/79, Loss 0.9813\n",
    "    Validation accuracy: 69.08%\n",
    "    Test accuracy: 69.47%\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.1397712833545"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
